{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Intrinsic Evaluation\n",
    "\n",
    "We measure the discrimination abilities of LLMs with four intrinsic metrics:\n",
    "\n",
    "1. **Discrimination Accuracy (Acc)**: Given a pair of correct and wrong programs, we calculate the percentage where the correct program obtains a higher discrimination score than the wrong one.\n",
    "\n",
    "2. **Classification Macro F1 (F1)**: We treat \"correct\" and \"wrong\" as two classes and compute the macro average of F1 scores on these two labels.\n",
    "\n",
    "3. **Hit@1 (H@1)**: Given a batch of candidate programs, we calculate the percentage where the highest-scoring candidate is correct.\n",
    "\n",
    "4. **Mean Reciprocal Rank (MRR)**: We compute the standard MRR score by the highest-ranking correct program in the batches.\n",
    "\n",
    "\n",
    "Legacy script: `scripts\\intrin_eval\\intrin_eval_text2sql_ft.sh`\n",
    "\n",
    "### Datasets:\n",
    "1. [Spider](https://yale-lily.github.io/spider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from accelerate.utils import set_seed\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import evaluate\n",
    "from evaluators.llm_evaluator import LLMEvaluator, LLMLoraEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_names =[\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"stabilityai/stable-code-3b\",\n",
    "    \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
    "]\n",
    "\n",
    "\n",
    "# generate lora model names\n",
    "lora_model_names = []\n",
    "for m in evaluator_names:\n",
    "   lora_model_names.append( m.split(\"/\")[1]+\"_spider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluator_name: stabilityai/stable-code-3b\n",
      "model_savename: stable-code-3b_spider\n"
     ]
    }
   ],
   "source": [
    "model_indx = 2 # choose the model to evaluate\n",
    "\n",
    "evaluator_name = evaluator_names[model_indx]\n",
    "model_savename = lora_model_names[model_indx]\n",
    "print(f\"evaluator_name: {evaluator_name}\")\n",
    "print(f\"model_savename: {model_savename}\")\n",
    "\n",
    "current_directory = os.getcwd() #parameters\n",
    "model_savedatapath = os.path.join(current_directory,f\"checkpts/{model_savename}/model\")\n",
    "evaluator_peft_dir = model_savedatapath\n",
    "\n",
    "seed = 42\n",
    "test_fname = \"data/spider_intrin_eval.json\"\n",
    "log_name = f\"{model_savename}_pro.json\"\n",
    "dataset_name = \"spider\"\n",
    "db_path =\"spider/database\"\n",
    "evaluation_config = \"evaluation_configs/pro.json\"\n",
    "\n",
    "\"\"\"\n",
    "yes_token_indx: \n",
    "    the index of the token in the vocabulary that corresponds to the \"Yes\" text.\n",
    "    CodeLlama-Instruct: \"No\" 1939 \"Yes\" 3869\n",
    "    TinyLlama: \"Yes\" 3869\n",
    "\"\"\"\n",
    "yes_token_indx=None#3869"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    set_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579eb7b69f454535af3ba5cc98b5d450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes token index: 4374\n"
     ]
    }
   ],
   "source": [
    "evaluator = LLMEvaluator(evaluator_name, db_path, device=\"cuda\",yes_token_indx=yes_token_indx)\n",
    "#evaluator = LLMLoraEvaluator(evaluator_name, evaluator_peft_dir, db_path, device=\"cuda\",yes_token_indx=yes_token_indx)\n",
    "\n",
    "yindx=evaluator.get_yes_token()\n",
    "print(f\"Yes token index: {yindx}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intrinsic_eval(evaluator, test_fname,evaluation_config,log_fname=\"\"):\n",
    "    # Load test data and evaluation configuration\n",
    "    test_data = json.load(open(test_fname))\n",
    "    evaluation_config = json.load(open(evaluation_config))\n",
    "\n",
    "    results = [] # Predicted labels\n",
    "    labels = [] # Ground truth labels\n",
    "    log = [] # Log data\n",
    "\n",
    "    # Pairwise selection accuracy\n",
    "    pairs_count = 0 # Pair count\n",
    "    pws_acc = 0 # Pairwise selection accuracy\n",
    "\n",
    "    # Example-level metrics\n",
    "    hit = 0 # Hit @ 1\n",
    "    mrr = 0 # Mean Reciprocal Rank\n",
    "\n",
    "    for ex in tqdm(test_data):\n",
    "        # Spider data format:\n",
    "        # ex has has the following keys:\n",
    "        # ['db_id', 'schema', 'question', 'sql', 'exec_res', 'top_n', 'top_n_exec_res', 'top_n_label']\n",
    "        \n",
    "        sql_completions = ex[\"top_n\"] # Candidate completions\n",
    "\n",
    "\n",
    "        # Evaluate the completions\n",
    "        scores = evaluator.score(ex[\"db_id\"], ex[\"question\"], sql_completions, evaluation_config)\n",
    "\n",
    "        scores = [-s for s in scores] # Negative scores for ranking\n",
    "\n",
    "        # Pairwise selection accuracy\n",
    "        # For each pair of completions, if the scores are different, check if the higher scoring completion is correct\n",
    "        for a in range(len(sql_completions)):\n",
    "            for b in range(a + 1, len(sql_completions)):\n",
    "                if ex[\"top_n_label\"][a] != ex[\"top_n_label\"][b]:\n",
    "                    pairs_count += 1\n",
    "                    if (\n",
    "                        (ex[\"top_n_label\"][a] == 1 and scores[a] > scores[b]) or\n",
    "                        (ex[\"top_n_label\"][b] == 1 and scores[b] > scores[a])\n",
    "                    ):\n",
    "                        pws_acc += 1\n",
    "\n",
    "\n",
    "        # Convert scores to binary classification labels\n",
    "        cls_res = [(1 if s > 0.5 else 0) for s in scores]\n",
    "        results += cls_res\n",
    "        labels += ex[\"top_n_label\"]\n",
    "\n",
    "        # Log data\n",
    "        ex_log = deepcopy(ex)\n",
    "        ex_log[\"pred_scores\"] = scores\n",
    "        ex_log[\"pred_labels\"] = cls_res\n",
    "        log.append(ex_log)\n",
    "\n",
    "        # Hit @ 1 and MRR\n",
    "        scores_labels = [(s, g) for s, g in zip(scores, ex[\"top_n_label\"])]\n",
    "        scores_labels.sort(key=lambda x: x[0], reverse=True)\n",
    "        reranked_labels = [tu[1] for tu in scores_labels]\n",
    "\n",
    "        if reranked_labels[0] == 1:\n",
    "            hit += 1\n",
    "        for idx, l in enumerate(reranked_labels):\n",
    "            if l == 1:\n",
    "                mrr += (1 / (idx + 1))\n",
    "                break\n",
    "\n",
    "    # Compute metrics\n",
    "\n",
    "    # acc_metric = evaluate.load(\"accuracy\")\n",
    "    # acc = acc_metric.compute(predictions=results, references=labels)[\"accuracy\"]\n",
    "\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    pos_f1 = f1_metric.compute(predictions=results, references=labels, pos_label=1)[\"f1\"]\n",
    "    neg_f1 = f1_metric.compute(predictions=results, references=labels, pos_label=0)[\"f1\"]\n",
    "    macro_f1 = (pos_f1 + neg_f1) / 2\n",
    "\n",
    "    # Print and log results\n",
    "    print(\n",
    "        \"Pair Count: {}\\nPWS Acc: {:<20.4f}\\nSQL Count: {}\\nPos F1: {:<20.4f}\\nNeg F1: {:<20.4f}\\nMacro F1: {:<20.4f}\\nHit @ 1: {:<20.4f}\\nMRR: {:<20.4f}\\n\".format(\n",
    "            pairs_count, pws_acc / pairs_count, len(results), pos_f1, neg_f1, macro_f1, hit / len(test_data), mrr / len(test_data)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Log data\n",
    "    if log_fname != \"\":\n",
    "        out = open(\"log/\" + log_fname, \"w+\", encoding=\"utf-8\")\n",
    "        json.dump(log, out, indent=2)\n",
    "        out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:00<?, ?it/s]c:\\Users\\fahim\\anaconda3\\envs\\r1\\lib\\site-packages\\transformers\\models\\stablelm\\modeling_stablelm.py:424: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 400/400 [01:15<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair Count: 409\n",
      "PWS Acc: 0.8313              \n",
      "SQL Count: 1221\n",
      "Pos F1: 0.0000              \n",
      "Neg F1: 0.7419              \n",
      "Macro F1: 0.3709              \n",
      "Hit @ 1: 0.6350              \n",
      "MRR: 0.6727              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "intrinsic_eval(evaluator, test_fname,evaluation_config,log_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spider Dataset Keys Explanation\n",
    "\n",
    "The Spider dataset contains various keys that help in evaluating text-to-SQL models. Below is an explanation of each key:\n",
    "\n",
    "- **`db_id`**: The unique identifier of the database for the given query. This indicates which database schema the question belongs to.\n",
    "\n",
    "- **`schema`**: The schema of the database, which includes information about tables, columns, and their relationships. This helps models understand the database structure.\n",
    "\n",
    "- **`question`**: The natural language question asked by the user.  \n",
    "  *Example:*  \n",
    "  *\"What is the name of the youngest employee?\"*\n",
    "\n",
    "- **`sql`**: The ground truth SQL query corresponding to the question.  \n",
    "  *Example:*  \n",
    "  ```sql\n",
    "  SELECT name FROM employees ORDER BY age ASC LIMIT 1;\n",
    "  ```\n",
    "\n",
    "- **`exec_res`**: The execution result of the ground truth SQL query. This contains the actual output of running the query on the database.\n",
    "\n",
    "- **`top_n`**: A list of the **top-N SQL completions** (candidate queries) generated by the model. These are ranked based on the model’s confidence scores.\n",
    "\n",
    "- **`top_n_exec_res`**: The execution results of the **top-N SQL completions**. These contain the actual database outputs of the model’s predicted queries.\n",
    "\n",
    "- **`top_n_label`**: A list of binary labels (`0` or `1`) for each SQL candidate in `top_n`.  \n",
    "  - `1` → The query is **correct** (produces the expected output).  \n",
    "  - `0` → The query is **incorrect** (does not produce the expected output)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
