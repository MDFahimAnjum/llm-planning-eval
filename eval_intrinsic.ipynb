{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Intrinsic Evaluation\n",
    "\n",
    "We measure the discrimination abilities of LLMs with four intrinsic metrics:\n",
    "\n",
    "1. **Discrimination Accuracy (Acc)**: Given a pair of correct and wrong programs, we calculate the percentage where the correct program obtains a higher discrimination score than the wrong one.\n",
    "\n",
    "2. **Classification Macro F1 (F1)**: We treat \"correct\" and \"wrong\" as two classes and compute the macro average of F1 scores on these two labels.\n",
    "\n",
    "3. **Hit@1 (H@1)**: Given a batch of candidate programs, we calculate the percentage where the highest-scoring candidate is correct.\n",
    "\n",
    "4. **Mean Reciprocal Rank (MRR)**: We compute the standard MRR score by the highest-ranking correct program in the batches.\n",
    "\n",
    "\n",
    "Legacy script: `scripts\\intrin_eval\\intrin_eval_text2sql_ft.sh`\n",
    "\n",
    "### Datasets:\n",
    "1. [Spider](https://yale-lily.github.io/spider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from evaluators.llm_evaluator import LLMEvaluator, LLMLoraEvaluator\n",
    "from utils.functions import set_seed_all\n",
    "from utils.functions import eval_intrinsic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_names =[\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"stabilityai/stable-code-3b\",\n",
    "    \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
    "]\n",
    "\n",
    "\n",
    "# generate lora model names\n",
    "lora_model_names = []\n",
    "for m in evaluator_names:\n",
    "   lora_model_names.append( m.split(\"/\")[1]+\"_spider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluator_name: stabilityai/stable-code-3b\n",
      "model_savename: stable-code-3b_spider\n"
     ]
    }
   ],
   "source": [
    "model_indx = 2 # choose the model to evaluate\n",
    "\n",
    "evaluator_name = evaluator_names[model_indx]\n",
    "model_savename = lora_model_names[model_indx]\n",
    "print(f\"evaluator_name: {evaluator_name}\")\n",
    "print(f\"model_savename: {model_savename}\")\n",
    "\n",
    "current_directory = os.getcwd() #parameters\n",
    "model_savedatapath = os.path.join(current_directory,f\"checkpts/{model_savename}/model\")\n",
    "evaluator_peft_dir = model_savedatapath\n",
    "\n",
    "seed = 42\n",
    "test_fname = \"data/spider_intrin_eval.json\"\n",
    "log_name = f\"{model_savename}_pro.json\"\n",
    "dataset_name = \"spider\"\n",
    "db_path =\"spider/database\"\n",
    "evaluation_config = \"evaluation_configs/pro.json\"\n",
    "\n",
    "\"\"\"\n",
    "yes_token_indx: \n",
    "    the index of the token in the vocabulary that corresponds to the \"Yes\" text.\n",
    "    CodeLlama-Instruct: \"No\" 1939 \"Yes\" 3869\n",
    "    TinyLlama: \"Yes\" 3869\n",
    "\"\"\"\n",
    "yes_token_indx=None#3869"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "set_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579eb7b69f454535af3ba5cc98b5d450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes token index: 4374\n"
     ]
    }
   ],
   "source": [
    "evaluator = LLMEvaluator(evaluator_name, db_path, device=\"cuda\",yes_token_indx=yes_token_indx)\n",
    "#evaluator = LLMLoraEvaluator(evaluator_name, evaluator_peft_dir, db_path, device=\"cuda\",yes_token_indx=yes_token_indx)\n",
    "\n",
    "yindx=evaluator.get_yes_token()\n",
    "print(f\"Yes token index: {yindx}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:00<?, ?it/s]c:\\Users\\fahim\\anaconda3\\envs\\r1\\lib\\site-packages\\transformers\\models\\stablelm\\modeling_stablelm.py:424: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 400/400 [01:15<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair Count: 409\n",
      "PWS Acc: 0.8313              \n",
      "SQL Count: 1221\n",
      "Pos F1: 0.0000              \n",
      "Neg F1: 0.7419              \n",
      "Macro F1: 0.3709              \n",
      "Hit @ 1: 0.6350              \n",
      "MRR: 0.6727              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_intrinsic(evaluator, test_fname,evaluation_config,log_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spider Dataset Keys Explanation\n",
    "\n",
    "The Spider dataset contains various keys that help in evaluating text-to-SQL models. Below is an explanation of each key:\n",
    "\n",
    "- **`db_id`**: The unique identifier of the database for the given query. This indicates which database schema the question belongs to.\n",
    "\n",
    "- **`schema`**: The schema of the database, which includes information about tables, columns, and their relationships. This helps models understand the database structure.\n",
    "\n",
    "- **`question`**: The natural language question asked by the user.  \n",
    "  *Example:*  \n",
    "  *\"What is the name of the youngest employee?\"*\n",
    "\n",
    "- **`sql`**: The ground truth SQL query corresponding to the question.  \n",
    "  *Example:*  \n",
    "  ```sql\n",
    "  SELECT name FROM employees ORDER BY age ASC LIMIT 1;\n",
    "  ```\n",
    "\n",
    "- **`exec_res`**: The execution result of the ground truth SQL query. This contains the actual output of running the query on the database.\n",
    "\n",
    "- **`top_n`**: A list of the **top-N SQL completions** (candidate queries) generated by the model. These are ranked based on the model’s confidence scores.\n",
    "\n",
    "- **`top_n_exec_res`**: The execution results of the **top-N SQL completions**. These contain the actual database outputs of the model’s predicted queries.\n",
    "\n",
    "- **`top_n_label`**: A list of binary labels (`0` or `1`) for each SQL candidate in `top_n`.  \n",
    "  - `1` → The query is **correct** (produces the expected output).  \n",
    "  - `0` → The query is **incorrect** (does not produce the expected output)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
