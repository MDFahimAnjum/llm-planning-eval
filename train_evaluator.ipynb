{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distilled R1 LoRA Finetune for Discriminator\n",
    "\n",
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch  # Required for tensor operations and GPU usage\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from utils.constants import TEMPLATE\n",
    "from utils.train_utils import jload\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer,TrainingArguments\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_names =[\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"stabilityai/stable-code-3b\",\n",
    "    \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
    "]\n",
    "\n",
    "\n",
    "# generate lora model names\n",
    "lora_model_names = []\n",
    "for m in evaluator_names:\n",
    "   lora_model_names.append( m.split(\"/\")[1]+\"_spider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "model_max_length = 300 # max length of the model\n",
    "data_path = \"./data/spider_evaluator_train_cls_exec.json\" # path to the data\n",
    "model_indx = 2 # choose the model to evaluate\n",
    "\n",
    "# autometically set the model name and the model save name\n",
    "model_name = evaluator_names[model_indx]\n",
    "model_savename = lora_model_names[model_indx]\n",
    "print(f\"evaluator_name: {model_name}\")\n",
    "print(f\"model_savename: {model_savename}\")\n",
    "current_directory = os.getcwd() #parameters\n",
    "train_savedatapath = os.path.join(current_directory,f\"checkpts/{model_savename}\")\n",
    "model_savedatapath = os.path.join(current_directory,f\"checkpts/{model_savename}/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  \n",
    "    llm_int8_threshold=6.0, \n",
    "    llm_int8_enable_fp32_cpu_offload=True  \n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,  # Explicitly set dtype to float16\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)  # Equivalent to prepare_model_for_int8_training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        model_max_length=model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False\n",
    "    )\n",
    "except:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        model_max_length=model_max_length,\n",
    "        padding_side=\"right\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Setting `pad_token` to `eos_token` for open-end generation.\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo prompting\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fahim\\anaconda3\\envs\\r1\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\fahim\\anaconda3\\envs\\r1\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:48: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 109])\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt\n",
    "prompt = \"\"\"\n",
    "Answer the following Yes/No question: Is the SQL correct given the utterance?\n",
    "\n",
    "-- Utterance: How many different countries are all the swimmers from?\n",
    "-- SQL:\n",
    "SELECT COUNT(DISTINCT nationality) FROM swimmer;\n",
    "-- Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize input\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Move tensors to GPU if available\n",
    "\"\"\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")  # Move tensors to GPU if available\n",
    "\"\"\"\n",
    "\n",
    "# Convert input tensors to float16, but keep integer tensors (like 'input_ids') as long\n",
    "inputs = {key: value.to(torch.float16) if value.dtype != torch.long and value.dtype != torch.int else value \n",
    "          for key, value in inputs.items()}\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    output = model.generate(**inputs, max_length=300, do_sample=True, temperature=0.6, num_return_sequences=5)\n",
    "\n",
    "print(output.shape)  # Output shape: (batch_size, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the SQL is correct given the utterance. The COUNT(DISTINCT nationality) FROM swimmer is a proper SELECT statement that counts the number of distinct values in the nationality column of the swimmer table.\n"
     ]
    }
   ],
   "source": [
    "# Decode and print generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#print(text)\n",
    "print(generated_text[len(prompt):])  # Print only the generated text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n"
     ]
    }
   ],
   "source": [
    "target_modules = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load supervised training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_sources_with_prompt = tokenizer(\n",
    "        sources,\n",
    "        max_length=1600 - 300,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    tokenized_targets = tokenizer(\n",
    "        targets,\n",
    "        max_length=300,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "    # you might need to convert to float 16:\n",
    "    # tokenized_sources_with_prompt\n",
    "    # tokenized_targets     \n",
    "\n",
    "    # Build the input and labels for causal LM\n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    for tokenized_source, tokenized_target in zip(\n",
    "        tokenized_sources_with_prompt['input_ids'],\n",
    "        tokenized_targets['input_ids']\n",
    "    ):\n",
    "        input_ids.append(torch.tensor(tokenized_source + tokenized_target))\n",
    "        labels.append(\n",
    "            torch.tensor([IGNORE_INDEX for _ in range(len(tokenized_source))] + copy.deepcopy(tokenized_target))\n",
    "        )\n",
    "\n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        list_data_dict = jload(data_path)\n",
    "\n",
    "        logging.warning(\"Formatting inputs...\")\n",
    "        sources = [\n",
    "            example[\"src\"]\n",
    "            for example in list_data_dict\n",
    "        ]\n",
    "        targets = [\n",
    "            f\"{example['tgt']}{tokenizer.eos_token}\"\n",
    "            for example in list_data_dict\n",
    "        ]\n",
    "\n",
    "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "        data_dict = preprocess(sources, targets, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,data_path,dev_data_path=None) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_path)\n",
    "    dev_dataset = None \n",
    "    if dev_data_path:\n",
    "        dev_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=dev_data_path)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=dev_dataset, data_collator=data_collator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    }
   ],
   "source": [
    "data_module = make_supervised_data_module(tokenizer=tokenizer, data_path=data_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=train_savedatapath,\n",
    "    overwrite_output_dir=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,# 1 in code, 128 in paper\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    save_total_limit=1,\n",
    "    push_to_hub=False,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type='cosine',\n",
    "    do_train=True,\n",
    "    do_eval=False,\n",
    "    eval_strategy='no',\n",
    "    save_strategy='steps',\n",
    "    save_steps=1000,\n",
    "    bf16=True,  # Enables bfloat16 precision (for NVIDIA Ampere+ GPUs)\n",
    "    tf32=True,  # Enables TF32 mode (for NVIDIA Ampere+ GPUs\n",
    "#    load_best_model_at_end=True,\n",
    "#    metric_for_best_model='loss', \n",
    "#    greater_is_better=False\n",
    ")\n",
    "\n",
    "# Ensure TF32 is enabled globally for better performance\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahim\\AppData\\Local\\Temp\\ipykernel_37060\\4214773630.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model, tokenizer=tokenizer,args=training_args, **data_module)\n",
      "c:\\Users\\fahim\\anaconda3\\envs\\r1\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\fahim\\anaconda3\\envs\\r1\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='623' max='623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [623/623 7:19:21, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.529300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure `use_cache=False` in model.config\n",
    "model.config.use_cache = False\n",
    "\n",
    "trainer = Trainer(model=model, tokenizer=tokenizer,args=training_args, **data_module)\n",
    "trainer.train()\n",
    "trainer.save_state()\n",
    "model.save_pretrained(model_savedatapath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
