{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distilled R1 LoRA Finetune for Discriminator\n",
    "\n",
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch  # Required for tensor operations and GPU usage\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from utils.constants import TEMPLATE\n",
    "from utils.train_utils import jload\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer,TrainingArguments\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_max_length = 300\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "#model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Example open-source 1B model\n",
    "data_path = \"./data/spider_evaluator_train_cls_exec.json\"\n",
    "model_savename = \"DeepSeek-R1-Distill-Qwen-1.5_spider\"\n",
    "current_directory = os.getcwd() #parameters\n",
    "\n",
    "train_savedatapath = os.path.join(current_directory,f\"checkpts/{model_savename}\")\n",
    "model_savedatapath = os.path.join(current_directory,f\"checkpts/{model_savename}/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  \n",
    "    llm_int8_threshold=6.0, \n",
    "    llm_int8_enable_fp32_cpu_offload=True  \n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,  # Explicitly set dtype to float16\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)  # Equivalent to prepare_model_for_int8_training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    model_max_length=model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "# Setting `pad_token` to `eos_token` for open-end generation.\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo prompting\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 300])\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt\n",
    "prompt = \"\"\"\n",
    "Answer the following Yes/No question: Is the SQL correct given the utterance?\n",
    "\n",
    "-- Utterance: How many different countries are all the swimmers from?\n",
    "-- SQL:\n",
    "SELECT COUNT(DISTINCT nationality) FROM swimmer;\n",
    "-- Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize input\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Move tensors to GPU if available\n",
    "\"\"\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")  # Move tensors to GPU if available\n",
    "\"\"\"\n",
    "\n",
    "# Convert input tensors to float16, but keep integer tensors (like 'input_ids') as long\n",
    "inputs = {key: value.to(torch.float16) if value.dtype != torch.long and value.dtype != torch.int else value \n",
    "          for key, value in inputs.items()}\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    output = model.generate(**inputs, max_length=300, do_sample=True, temperature=0.6, num_return_sequences=5)\n",
    "\n",
    "print(output.shape)  # Output shape: (batch_size, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n",
      "</think>\n",
      "\n",
      "To determine if the SQL query is correct given the utterance, let's analyze the SQL and the query.\n",
      "\n",
      "**SQL Query:**\n",
      "```sql\n",
      "SELECT COUNT(DISTINCT nationality) FROM swimmer;\n",
      "```\n",
      "\n",
      "**Utterance:**\n",
      "\"How many different countries are all the swimmers from?\"\n",
      "\n",
      "**Analysis:**\n",
      "The SQL query `COUNT(DISTINCT nationality)` is intended to count the number of unique nationalities among the swimmers. However, the query does not explicitly count the number of different countries. It only counts the number of unique nationalities. Therefore, the SQL query is incomplete and does not address the question about the number of different countries.\n",
      "\n",
      "**Answer:**\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "# Decode and print generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#print(text)\n",
    "print(generated_text[len(prompt):])  # Print only the generated text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,089,536 || all params: 1,778,177,536 || trainable%: 0.0613\n"
     ]
    }
   ],
   "source": [
    "target_modules = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load supervised training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_sources_with_prompt = tokenizer(\n",
    "        sources,\n",
    "        max_length=1600 - 300,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    tokenized_targets = tokenizer(\n",
    "        targets,\n",
    "        max_length=300,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "    # you might need to convert to float 16:\n",
    "    # tokenized_sources_with_prompt\n",
    "    # tokenized_targets     \n",
    "\n",
    "    # Build the input and labels for causal LM\n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    for tokenized_source, tokenized_target in zip(\n",
    "        tokenized_sources_with_prompt['input_ids'],\n",
    "        tokenized_targets['input_ids']\n",
    "    ):\n",
    "        input_ids.append(torch.tensor(tokenized_source + tokenized_target))\n",
    "        labels.append(\n",
    "            torch.tensor([IGNORE_INDEX for _ in range(len(tokenized_source))] + copy.deepcopy(tokenized_target))\n",
    "        )\n",
    "\n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        list_data_dict = jload(data_path)\n",
    "\n",
    "        logging.warning(\"Formatting inputs...\")\n",
    "        sources = [\n",
    "            example[\"src\"]\n",
    "            for example in list_data_dict\n",
    "        ]\n",
    "        targets = [\n",
    "            f\"{example['tgt']}{tokenizer.eos_token}\"\n",
    "            for example in list_data_dict\n",
    "        ]\n",
    "\n",
    "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "        data_dict = preprocess(sources, targets, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,data_path,dev_data_path=None) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_path)\n",
    "    dev_dataset = None \n",
    "    if dev_data_path:\n",
    "        dev_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=dev_data_path)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=dev_dataset, data_collator=data_collator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    }
   ],
   "source": [
    "data_module = make_supervised_data_module(tokenizer=tokenizer, data_path=data_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=train_savedatapath,\n",
    "    overwrite_output_dir=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,# 1 in code, 128 in paper\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    save_total_limit=1,\n",
    "    push_to_hub=False,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type='cosine',\n",
    "    do_train=True,\n",
    "    do_eval=False,\n",
    "    eval_strategy='no',\n",
    "    save_strategy='steps',\n",
    "    save_steps=1000,\n",
    "    bf16=True,  # Enables bfloat16 precision (for NVIDIA Ampere+ GPUs)\n",
    "    tf32=True,  # Enables TF32 mode (for NVIDIA Ampere+ GPUs\n",
    "#    load_best_model_at_end=True,\n",
    "#    metric_for_best_model='loss', \n",
    "#    greater_is_better=False\n",
    ")\n",
    "\n",
    "# Ensure TF32 is enabled globally for better performance\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure `use_cache=False` in model.config\n",
    "model.config.use_cache = False\n",
    "\n",
    "trainer = Trainer(model=model, tokenizer=tokenizer,args=training_args, **data_module)\n",
    "trainer.train()\n",
    "trainer.save_state()\n",
    "model.save_pretrained(model_savedatapath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
